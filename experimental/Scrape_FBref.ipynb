{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merge and Scrape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'pass types': {'merged': 'pass types_checkpoint.csv', 'cont': 'pass types_checkpoint_last195.csv'}, 'possession': {'merged': 'possession_checkpoint.csv', 'cont': 'possession_checkpoint_last195.csv'}, 'goalkeeping': {'merged': 'goalkeeping_checkpoint.csv', 'cont': 'goalkeeping_checkpoint_last195.csv'}, 'passing': {'cont': 'passing_checkpoint_last195.csv', 'merged': 'passing_checkpoint.csv'}, 'shooting': {'merged': 'shooting_checkpoint.csv', 'cont': 'shooting_checkpoint_last195.csv'}, 'miscellaneous stats': {'merged': 'miscellaneous stats_checkpoint.csv', 'cont': 'miscellaneous stats_checkpoint_last195.csv'}, 'advanced goalkeeping': {'merged': 'advanced goalkeeping_checkpoint.csv', 'cont': 'advanced goalkeeping_checkpoint_last195.csv'}, 'goal and shot creation': {'merged': 'goal and shot creation_checkpoint.csv', 'cont': 'goal and shot creation_checkpoint_last195.csv'}, 'defensive actions': {'cont': 'defensive actions_checkpoint_last195.csv', 'merged': 'defensive actions_checkpoint.csv'}, 'playing time': {'cont': 'playing time_checkpoint_last195.csv', 'merged': 'playing time_checkpoint.csv'}}\n",
            "Merged pass types_checkpoint.csv and pass types_checkpoint_last195.csv -> checkpoints/pass types_checkpoint.csv\n",
            "Merged possession_checkpoint.csv and possession_checkpoint_last195.csv -> checkpoints/possession_checkpoint.csv\n",
            "Merged goalkeeping_checkpoint.csv and goalkeeping_checkpoint_last195.csv -> checkpoints/goalkeeping_checkpoint.csv\n",
            "Merged passing_checkpoint.csv and passing_checkpoint_last195.csv -> checkpoints/passing_checkpoint.csv\n",
            "Merged shooting_checkpoint.csv and shooting_checkpoint_last195.csv -> checkpoints/shooting_checkpoint.csv\n",
            "Merged miscellaneous stats_checkpoint.csv and miscellaneous stats_checkpoint_last195.csv -> checkpoints/miscellaneous stats_checkpoint.csv\n",
            "Merged advanced goalkeeping_checkpoint.csv and advanced goalkeeping_checkpoint_last195.csv -> checkpoints/advanced goalkeeping_checkpoint.csv\n",
            "Merged goal and shot creation_checkpoint.csv and goal and shot creation_checkpoint_last195.csv -> checkpoints/goal and shot creation_checkpoint.csv\n",
            "Merged defensive actions_checkpoint.csv and defensive actions_checkpoint_last195.csv -> checkpoints/defensive actions_checkpoint.csv\n",
            "Merged playing time_checkpoint.csv and playing time_checkpoint_last195.csv -> checkpoints/playing time_checkpoint.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def merge_checkpoint_files(directory):\n",
        "    \"\"\"Merges _cont-755.csv files with their corresponding _checkpoint.csv files.\"\"\"\n",
        "    files = os.listdir(directory)\n",
        "\n",
        "    file_groups = {}\n",
        "    for file in files:\n",
        "        if file.endswith(\".csv\"):\n",
        "            base_name = file.replace(\"_checkpoint.csv\", \"\").replace(\n",
        "                \"_checkpoint_last195.csv\", \"\")\n",
        "            if base_name not in file_groups:\n",
        "                file_groups[base_name] = {}\n",
        "            if \"195\" in file:\n",
        "                file_groups[base_name][\"cont\"] = file\n",
        "            else:\n",
        "                file_groups[base_name][\"merged\"] = file\n",
        "    print(file_groups)\n",
        "\n",
        "    for base_name, group in file_groups.items():\n",
        "        if \"cont\" in group and \"merged\" in group:\n",
        "            merged_path = os.path.join(directory, group[\"merged\"])\n",
        "            cont_path = os.path.join(directory, group[\"cont\"])\n",
        "\n",
        "            df_merged = pd.read_csv(merged_path)\n",
        "            df_cont = pd.read_csv(cont_path)\n",
        "\n",
        "            merged_df = pd.concat([df_merged, df_cont], ignore_index=True)\n",
        "            merged_path = os.path.join(\n",
        "                directory, f\"{base_name}_checkpoint.csv\")\n",
        "\n",
        "            merged_df.to_csv(merged_path, index=False)\n",
        "            print(\n",
        "                f\"Merged {group['merged']} and {group['cont']} -> {merged_path}\")\n",
        "\n",
        "\n",
        "merge_checkpoint_files(\"checkpoints\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Player SCRAPER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from io import StringIO\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "import glob\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "stats_list = ('standard', 'goalkeeping', 'advanced goalkeeping', 'shooting', 'passing', 'pass types',\n",
        "              'goal and shot creation', 'defensive actions', 'possession', 'playing time', 'miscellaneous stats')\n",
        "\n",
        "\n",
        "def available_stats():\n",
        "    \"\"\" Returns the available stats to scrape \"\"\"\n",
        "    return stats_list\n",
        "\n",
        "\n",
        "def get_player_stats_from_URL(url: str, stat: str):\n",
        "    \"\"\" Get player stats from FBref.com, using the given URL\n",
        "    url: the url to get the stats from\n",
        "    stat: the stat to get, must be one of the available stats\n",
        "\n",
        "    returns: pandas dataframe of the stats\n",
        "    \"\"\"\n",
        "    if stat not in stats_list:\n",
        "        raise ValueError(f'stat must be one of {stats_list}')\n",
        "\n",
        "    table, row = _get_table_from_URL(url, stat)\n",
        "    df = _get_dataframe(table, row, url.split(\"/\")[-1])\n",
        "    return df\n",
        "\n",
        "\n",
        "def _get_table_from_URL(url, stat):\n",
        "    # print(f'Getting data from {url}...')\n",
        "    res = requests.get(url, timeout=10)\n",
        "    comm = re.compile('<!--|-->')\n",
        "    soup = bs(comm.sub('', res.text))\n",
        "    table = soup.find('div', {'id': f'all_stats_{stat}'}).find(\"table\")\n",
        "    tbod = table.find(\"tbody\").find_all(\"tr\")\n",
        "    # print(len(tbod))\n",
        "    # print('Done.')\n",
        "    # print(table)\n",
        "    return table.prettify(), len(tbod)\n",
        "\n",
        "\n",
        "def get_all_player_stats_from_URL(url: str):\n",
        "    \"\"\" Get player stats from FBref.com, using the given URL\n",
        "    url: the url to get the stats fromKnowing ki you are\n",
        "\n",
        "    returns: pandas dataframe of the stats\n",
        "    \"\"\"\n",
        "    tables = _gel_all_tables_from_URL(url)\n",
        "    if tables == -1:\n",
        "        return -1\n",
        "    dfs = {}\n",
        "    for table in tables:\n",
        "        if table.caption.text.lower().split(\":\")[0] not in stats_list:\n",
        "            continue\n",
        "        df = _get_dataframe(table.prettify(), len(\n",
        "            table.find(\"tbody\").find_all(\"tr\")), url.split(\"/\")[-1])\n",
        "        dfs[table.caption.text.lower().split(\":\")[0]] = df\n",
        "    # print(f\"Processed {url.split(\"/\")[-1]}.\")\n",
        "    return dfs\n",
        "\n",
        "\n",
        "def _gel_all_tables_from_URL(url):\n",
        "    try:\n",
        "        res = requests.get(url, timeout=20)\n",
        "    except requests.ReadTimeout as e:\n",
        "        print(f\"ReadTimeout: {e}\")\n",
        "        time.sleep(10)\n",
        "        return _gel_all_tables_from_URL(url)\n",
        "    comm = re.compile('<!--|-->')\n",
        "    soup = bs(comm.sub('', res.text))\n",
        "    tables = soup.find_all('table', {'class': 'stats_table'})\n",
        "    if not tables:\n",
        "        print(\"No tables found for {}. Got error - {}\".format(url, res.status_code))\n",
        "        return -1\n",
        "    return tables\n",
        "\n",
        "\n",
        "def _get_dataframe(table, row, name):\n",
        "    df = pd.read_html(StringIO(table))\n",
        "    df = df[0]\n",
        "\n",
        "    # delete the last column (Rk, Match)\n",
        "    df = df.iloc[:, :-1]\n",
        "\n",
        "    # keep only the second value for the headers\n",
        "\n",
        "    df.columns = [h[1] for h in df.columns]\n",
        "\n",
        "    # only keep the part after space for 'Nation'\n",
        "    df['Country'] = df['Country'].apply(\n",
        "        lambda x: str(x).rsplit(' ', maxsplit=1)[-1])\n",
        "\n",
        "    # delete rows with the column names\n",
        "    df = df[df[df.columns[0]] != df.columns[0]]\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    df.drop([\"Age\", \"Squad\", \"Country\", \"Comp\", \"LgRank\"], axis=1, inplace=True)\n",
        "\n",
        "    # convert all numeric columns to numeric\n",
        "    df = df.apply(pd.to_numeric, errors='ignore')\n",
        "    df = df.iloc[[row]]\n",
        "    df.insert(0, \"Player Name\", name)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2596, 2)\n"
          ]
        }
      ],
      "source": [
        "merged_df = pd.read_csv(\"../data/urls/merged_urls.csv\")\n",
        "merged_df = merged_df.drop_duplicates()\n",
        "merged_df = merged_df.reset_index(drop=True)\n",
        "print(merged_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scrape Player data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'merged_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m CHECKPOINT_INTERVAL = \u001b[32m50\u001b[39m\n\u001b[32m      5\u001b[39m stats_dict = {stat: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m stat \u001b[38;5;129;01min\u001b[39;00m stats_list}\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m fdf = \u001b[43mmerged_df\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33murls\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m196\u001b[39m:]\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(fdf), start=\u001b[32m1\u001b[39m):\n\u001b[32m      9\u001b[39m     dfs = get_all_player_stats_from_URL(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://fbref.com\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'merged_df' is not defined"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "CHECKPOINT_INTERVAL = 50\n",
        "\n",
        "stats_dict = {stat: None for stat in stats_list}\n",
        "fdf = merged_df[\"urls\"][-196:]\n",
        "\n",
        "for i, url in enumerate(tqdm(fdf), start=1):\n",
        "    dfs = get_all_player_stats_from_URL(f\"https://fbref.com{url}\")\n",
        "    while dfs == -1:\n",
        "        print(\"Error 429... Breaking\")\n",
        "        time.sleep(10*60)\n",
        "        dfs = get_all_player_stats_from_URL(f\"https://fbref.com{url}\")\n",
        "    for stat in list(dfs.keys()):\n",
        "        if stats_dict[stat] is None:\n",
        "            stats_dict[stat] = dfs[stat]\n",
        "        else:\n",
        "            stats_dict[stat] = pd.concat([stats_dict[stat], dfs[stat]], axis=0)\n",
        "\n",
        "    # print(url.split(\"/\")[-1], \" : \",stats_dict)\n",
        "\n",
        "    if i % CHECKPOINT_INTERVAL == 0:\n",
        "        print(f\"Checkpoint reached: Processed {i} rows. Saving progress...\")\n",
        "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "        for stat, df in stats_dict.items():\n",
        "            if df is not None:\n",
        "                # print(df)\n",
        "                df.to_csv(\n",
        "                    f\"checkpoints/{stat}_checkpoint.csv\", index=False)\n",
        "                # print(f\"Checkpoint saved for {stat}!\")\n",
        "        print(\"Checkpoint saved!\")\n",
        "        time.sleep(9)\n",
        "\n",
        "    time.sleep(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title remove duplicates and store them with same name\n",
        "\n",
        "for file in glob.glob(\"checkpoints/*.csv\"):\n",
        "    df = pd.read_csv(file)\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df.to_csv(file, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change positions in player data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrames saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"merged-tm-fb.csv\")\n",
        "\n",
        "df.loc[df['position'] == 'Second Striker', 'position'] = 'Attacking Midfield'\n",
        "df.loc[df['position'] == 'Left Midfield', 'position'] = 'Left Winger'\n",
        "df.loc[df['position'] == 'Right Midfield', 'position'] = 'Right Winger'\n",
        "\n",
        "positions = df['position'].unique()\n",
        "for pos in positions:\n",
        "    # Create a DataFrame for each position\n",
        "    df_pos = df[df['position'] == pos]\n",
        "\n",
        "    file_name = f\"player_data_{pos}.csv\"\n",
        "    df_pos.to_csv(f\"../data/position data/24-25/{file_name}\", index=False)\n",
        "\n",
        "print(\"DataFrames saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add league to player data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "Add league to player data"
        ]
      },
      "outputs": [],
      "source": [
        "# @title Add league to player data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# leagues\n",
        "premier = pd.read_csv(\"../data/urls-Premier League.csv\")\n",
        "laliga = pd.read_csv(\"../data/urls-La Liga.csv\")\n",
        "seriea = pd.read_csv(\"../data/urls-Serie A.csv\")\n",
        "bundesliga = pd.read_csv(\"../data/urls-Bundesliga.csv\")\n",
        "ligue1 = pd.read_csv(\"../data/urls-Ligue 1.csv\")\n",
        "\n",
        "# merged df\n",
        "df = pd.read_csv(\"../data/merged-tm-fb.csv\")\n",
        "\n",
        "# Create a dictionary to map player URLs to league names\n",
        "league_dict = {}\n",
        "\n",
        "for league_df, league_name in zip([premier, laliga, seriea, bundesliga, ligue1],\n",
        "                                  ['Premier League', 'La Liga', 'Serie A', 'Bundesliga', 'Ligue 1']):\n",
        "    for url in league_df['urls']:\n",
        "        league_dict[url.split(\"/\")[-1].replace(\"-\", \" \")] = league_name\n",
        "\n",
        "# Add the league column to df\n",
        "df['league'] = df['name'].map(league_dict)\n",
        "df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
        "df.head()\n",
        "\n",
        "df.to_csv(\"../data/merged-tm-fb.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merge all stats data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV files successfully combined!\n"
          ]
        }
      ],
      "source": [
        "# @title: Combine columns and rename cols with same name different values\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "csv_files = glob.glob(\"../data/season wise data/2024-2025/*.csv\")\n",
        "\n",
        "dfs = []\n",
        "cols = []\n",
        "fdf = pd.DataFrame()\n",
        "skip = [\"Player\", \"Nation\", \"Pos\", \"Squad\", \"Comp\", \"Age\", \"Born\", \"MP\", \"Starts\", \"Min\", \"90s\", \"Gls\", \"Ast\", \"G+A\", \"G-PK\", \"PK\", \"PKatt\", \"CrdY\",\n",
        "        \"CrdR\", \"xG\", \"npxG\", \"xAG\", \"npxG+xAG\", \"PrgC\", \"PrgP\", \"PrgR\", \"Gls\", \"Ast\",  \"G+A\", \"G-PK\", \"G+A-PK\", \"xG\", \"xAG\", \"xG+xAG\", \"npxG\", \"npxG+xAG\"]\n",
        "\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file)\n",
        "    df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
        "    for col in df.columns:\n",
        "        if col in skip:\n",
        "            continue\n",
        "        if col in cols:\n",
        "            if df[col].equals(fdf[col]):\n",
        "                continue\n",
        "            else:\n",
        "                df.rename(\n",
        "                    columns={col: f\"{col}_{file.split('/')[-1].split('.')[0]}\"}, inplace=True)\n",
        "        cols.append(col)\n",
        "    dfs.append(df)\n",
        "    fdf = pd.concat([fdf, df], axis=0, ignore_index=True)\n",
        "\n",
        "final_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "\n",
        "final_df = final_df.groupby(\"Player\", as_index=False).first()\n",
        "\n",
        "final_df.to_csv(\"combined_players_data.csv\", index=False)\n",
        "\n",
        "print(\"CSV files successfully combined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duplicate Columns: []\n"
          ]
        }
      ],
      "source": [
        "# cpdf = pd.read_csv(\"combined_players_data.csv\")\n",
        "# tmdf = pd.read_csv(\"../data/tm data/merged_tm.csv\")\n",
        "# cpdf = cpdf.rename(columns = {\"Player\": \"name\"})\n",
        "# cpdf[\"name\"] = cpdf.name.apply(lambda x: x.replace(\"-\", \" \"))\n",
        "# tmdf = tmdf.drop('Unnamed: 0', axis = 1)\n",
        "# df = cpdf.merge(tmdf, on=\"name\", how=\"inner\")\n",
        "# duplicate_columns = df.columns[df.columns.duplicated()].tolist()\n",
        "# print(\"Duplicate Columns:\", duplicate_columns)\n",
        "# dfdrop = df.T.drop_duplicates().T\n",
        "# df.to_csv(\"merged-tm-fb.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TEAM SCRAPER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from io import StringIO\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from bs4 import NavigableString\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "import glob\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "stats_list = {'standard': 'standard', 'keepers': 'keeper', 'keepersadv': \"keeper_adv\", 'shooting': \"shooting\", 'passing': \"passing\", 'passing_types': \"passing_types\",\n",
        "              'gca': \"gca\", 'defense': \"defense\", 'possession': \"possession\", 'playingtime': \"playing_time\", 'misc': \"misc\"}\n",
        "\n",
        "\n",
        "class TeamScraper():\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def get_dfs(self):\n",
        "        self.dfs = self.get_all_team_stats_from_URL(self.url)\n",
        "        return self.dfs\n",
        "\n",
        "    def get_all_team_stats_from_URL(self, url: str):\n",
        "        \"\"\" Get player stats from FBref.com, using the given URL\n",
        "        url: the url to get the stats fromKnowing ki you are\n",
        "\n",
        "        returns: pandas dataframe of the stats\n",
        "        \"\"\"\n",
        "        tables = self._gel_all_team_tables(url)\n",
        "        if tables == -1:\n",
        "            return -1\n",
        "        dfs = {}\n",
        "        for table in tables:\n",
        "            # if table.caption.text.lower().split(\":\")[0] not in stats_list:\n",
        "            #     continue\n",
        "            df = self._get_dataframe(table.prettify())\n",
        "            dfs[table.caption.text.lower().split(\":\")[0]] = df\n",
        "        # print(f\"Processed {url.split(\"/\")[-1]}.\")\n",
        "        return dfs\n",
        "\n",
        "    def _gel_all_team_tables(self, url):\n",
        "        try:\n",
        "            res = requests.get(url, timeout=20)\n",
        "        except requests.ReadTimeout as e:\n",
        "            print(f\"ReadTimeout: {e}\")\n",
        "            time.sleep(10)\n",
        "            return self._gel_all_team_tables(url)\n",
        "        # comm = re.compile('<!--|-->')\n",
        "        # self.soup = bs(comm.sub('', res.text))\n",
        "        self.soup = bs(res.text)\n",
        "        tables = []\n",
        "        for div in self.soup.find_all('div', id=lambda x: x and 'for' in x):\n",
        "            table = div.find('table', class_='stats_table')\n",
        "            if table:\n",
        "                tables.append(table)\n",
        "        if not tables:\n",
        "            print(\"No tables found for {}. Got error - {}\".format(url, res.status_code))\n",
        "            return -1\n",
        "        return tables\n",
        "\n",
        "    def _get_dataframe(self, table):\n",
        "        df = pd.read_html(StringIO(table))\n",
        "        df = df[0]\n",
        "\n",
        "        # delete the last column (Rk, Match)\n",
        "        df = df.iloc[:, :-1]\n",
        "\n",
        "        # keep only the second value for the headers\n",
        "        # print(df.columns)\n",
        "        df.columns = [h[0] for h in df.columns]\n",
        "\n",
        "        # delete rows with the column names\n",
        "        # df = df[df[df.columns[0]] != df.columns[0]]\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "        df.drop([\"Age\", \"Squad\", \"Country\", \"Comp\", \"LgRank\"],\n",
        "                axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "        # convert all numeric columns to numeric\n",
        "        df = df.apply(pd.to_numeric, errors='ignore')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def get_league_leaders(self):\n",
        "        res = requests.get(self.url, timeout=20)\n",
        "        comm = re.compile('<!--|-->')\n",
        "        soup = bs(comm.sub('', res.text))\n",
        "        leaders = soup.find('div', {'id': 'div_leaders'})\n",
        "        if not leaders:\n",
        "            print(\"No leaders found for {}. Got error\".format(self.url))\n",
        "            return -1\n",
        "        leaders_div = leaders.find_all('table', {'class': 'columns'})\n",
        "        names = [leader.caption.text for leader in leaders_div]\n",
        "        # print(leaders_div)\n",
        "        leaders = [leader.prettify()\n",
        "                   for leader in leaders if not isinstance(leader, NavigableString)]\n",
        "        leaders = [pd.read_html(StringIO(leader)) for leader in leaders]\n",
        "        leaders = [leader[0].rename(columns={leader[0].columns[0]: \"Rank\", leader[0].columns[1]\n",
        "                                    : \"Player\", leader[0].columns[2]: \"Value\"}) for leader in leaders]\n",
        "        return {names[i]: leaders[i] for i in range(len(names))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scrape teams data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "League: Premier League\n",
            "URL: https://fbref.com/en/comps/9/Premier-League-Stats\n",
            "\n",
            "df saved for squad standard stats 2024-2025 premier league table!\n",
            "df saved for squad goalkeeping 2024-2025 premier league table!\n",
            "df saved for squad advanced goalkeeping 2024-2025 premier league table!\n",
            "df saved for squad shooting 2024-2025 premier league table!\n",
            "df saved for squad passing 2024-2025 premier league table!\n",
            "df saved for squad pass types 2024-2025 premier league table!\n",
            "df saved for squad goal and shot creation 2024-2025 premier league table!\n",
            "df saved for squad defensive actions 2024-2025 premier league table!\n",
            "df saved for squad possession 2024-2025 premier league table!\n",
            "df saved for squad playing time 2024-2025 premier league table!\n",
            "df saved for squad miscellaneous stats 2024-2025 premier league table!\n",
            "leaders saved for PSxG-GA/90!\n",
            "League: La Liga\n",
            "URL: https://fbref.com/en/comps/12/La-Liga-Stats\n",
            "\n",
            "df saved for squad standard stats 2024-2025 la liga table!\n",
            "df saved for squad goalkeeping 2024-2025 la liga table!\n",
            "df saved for squad advanced goalkeeping 2024-2025 la liga table!\n",
            "df saved for squad shooting 2024-2025 la liga table!\n",
            "df saved for squad passing 2024-2025 la liga table!\n",
            "df saved for squad pass types 2024-2025 la liga table!\n",
            "df saved for squad goal and shot creation 2024-2025 la liga table!\n",
            "df saved for squad defensive actions 2024-2025 la liga table!\n",
            "df saved for squad possession 2024-2025 la liga table!\n",
            "df saved for squad playing time 2024-2025 la liga table!\n",
            "df saved for squad miscellaneous stats 2024-2025 la liga table!\n",
            "leaders saved for PSxG-GA/90!\n",
            "League: Bundesliga\n",
            "URL: https://fbref.com/en/comps/20/Bundesliga-Stats\n",
            "\n",
            "df saved for squad standard stats 2024-2025 bundesliga table!\n",
            "df saved for squad goalkeeping 2024-2025 bundesliga table!\n",
            "df saved for squad advanced goalkeeping 2024-2025 bundesliga table!\n",
            "df saved for squad shooting 2024-2025 bundesliga table!\n",
            "df saved for squad passing 2024-2025 bundesliga table!\n",
            "df saved for squad pass types 2024-2025 bundesliga table!\n",
            "df saved for squad goal and shot creation 2024-2025 bundesliga table!\n",
            "df saved for squad defensive actions 2024-2025 bundesliga table!\n",
            "df saved for squad possession 2024-2025 bundesliga table!\n",
            "df saved for squad playing time 2024-2025 bundesliga table!\n",
            "df saved for squad miscellaneous stats 2024-2025 bundesliga table!\n",
            "leaders saved for PSxG-GA/90!\n",
            "League: Serie A\n",
            "URL: https://fbref.com/en/comps/11/Serie-A-Stats\n",
            "\n",
            "df saved for squad standard stats 2024-2025 serie a table!\n",
            "df saved for squad goalkeeping 2024-2025 serie a table!\n",
            "df saved for squad advanced goalkeeping 2024-2025 serie a table!\n",
            "df saved for squad shooting 2024-2025 serie a table!\n",
            "df saved for squad passing 2024-2025 serie a table!\n",
            "df saved for squad pass types 2024-2025 serie a table!\n",
            "df saved for squad goal and shot creation 2024-2025 serie a table!\n",
            "df saved for squad defensive actions 2024-2025 serie a table!\n",
            "df saved for squad possession 2024-2025 serie a table!\n",
            "df saved for squad playing time 2024-2025 serie a table!\n",
            "df saved for squad miscellaneous stats 2024-2025 serie a table!\n",
            "leaders saved for PSxG-GA/90!\n",
            "League: Ligue 1\n",
            "URL: https://fbref.com/en/comps/13/Ligue-1-Stats\n",
            "\n",
            "df saved for squad standard stats 2024-2025 ligue 1 table!\n",
            "df saved for squad goalkeeping 2024-2025 ligue 1 table!\n",
            "df saved for squad advanced goalkeeping 2024-2025 ligue 1 table!\n",
            "df saved for squad shooting 2024-2025 ligue 1 table!\n",
            "df saved for squad passing 2024-2025 ligue 1 table!\n",
            "df saved for squad pass types 2024-2025 ligue 1 table!\n",
            "df saved for squad goal and shot creation 2024-2025 ligue 1 table!\n",
            "df saved for squad defensive actions 2024-2025 ligue 1 table!\n",
            "df saved for squad possession 2024-2025 ligue 1 table!\n",
            "df saved for squad playing time 2024-2025 ligue 1 table!\n",
            "df saved for squad miscellaneous stats 2024-2025 ligue 1 table!\n",
            "leaders saved for PSxG-GA/90!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "URL = \"https://fbref.com/en/comps/{}/{}-Stats\"\n",
        "\n",
        "LEAGUES = {\n",
        "    \"Premier League\": 9,\n",
        "    \"La Liga\": 12,\n",
        "    \"Bundesliga\": 20,\n",
        "    \"Serie A\": 11,\n",
        "    \"Ligue 1\": 13,\n",
        "}\n",
        "\n",
        "for k, v in LEAGUES.items():\n",
        "    print(f\"League: {k}\")\n",
        "    print(f\"URL: {URL.format(v, k.replace(' ', '-'))}\")\n",
        "    print()\n",
        "    ts = TeamScraper(URL.format(v, k.replace(' ', '-')))\n",
        "    dfs = ts.get_dfs()\n",
        "    for key, value in dfs.items():\n",
        "        if value is not None:\n",
        "            # if not os.path.exists(f\"../data/team data/{k}/{key}.csv\"):\n",
        "            os.makedirs(f\"../data/team data/{k}\", exist_ok=True)\n",
        "            value.to_csv(\n",
        "                f\"../data/team data/{k}/{key}.csv\", index=False)\n",
        "            print(f\"df saved for {key}!\")\n",
        "    leaders = ts.get_league_leaders()\n",
        "    os.makedirs(f\"../data/team data/{k}/Leaders/\", exist_ok=True)\n",
        "    for name, leader_df in leaders.items():\n",
        "        if leader_df is not None:\n",
        "            try:\n",
        "                leader_df.to_csv(\n",
        "                    f\"../data/team data/{k}/Leaders/{name.replace(\"/\", \"-\")}.csv\", index=False)\n",
        "            except:\n",
        "                print(f\"Error saving {name}...\")\n",
        "                continue\n",
        "        else:\n",
        "            print(f\"No data for {name}...\")\n",
        "\n",
        "    print(f\"leaders saved for {name}!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Scrape_FBref.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
