# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xwysGBo-tD1da9UB2p5msiZuUI4QvWmn

# Price Prediction

Problem Statement:

---

## Imports
"""

import pandas as pd
import numpy as np
from glob import glob
import warnings
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.preprocessing import LabelEncoder
from scipy.stats.mstats import winsorize
import lightgbm as lgb
from sklearn.ensemble import RandomForestRegressor
import re
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, HistGradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import StackingRegressor, VotingRegressor, BaggingRegressor

"""---

#### Warning Suppresion
"""

warnings.filterwarnings('ignore')

"""#### Defining the File Path

"""

file_path = "/content/merged-tm-fb.csv"

"""---

## Exploration
"""

final_df = pd.read_csv(file_path)
# final_df.drop(final_df.columns[0], axis=1, inplace=True)
final_df

list(final_df.columns)

final_df.describe()

"""---

## Filtering into smaller datasets
"""

final_df['position'].unique()

def split_by_position(csv_path: str = None, df=None):
    """
    Reads a CSV file and splits the DataFrame into four DataFrames based on the 'position' column.

    - Rows with 'Back' in the position are added to the defenders DataFrame.
    - Rows with 'Midfield' in the position are added to the midfielders DataFrame.
    - Rows with 'Goalkeeper' in the position are added to the goalkeepers DataFrame.
    - Rows with 'Winger' or 'Forward' in the position are added to the forwards DataFrame.

    Parameters:
        csv_path (str): Path to the CSV file.

    Returns:
        tuple: A tuple of DataFrames in the order (defenders_df, midfielders_df, goalkeepers_df, forwards_df).
    """
    if csv_path:
        df = pd.read_csv(csv_path)

    df['position'] = df['position'].astype(str)

    defenders_df = df[df['position'].str.contains(
        "Back", case=False, na=False)].copy()
    midfielders_df = df[df['position'].str.contains(
        "Midfield", case=False, na=False)].copy()
    goalkeepers_df = df[df['position'].str.contains(
        "Goalkeeper", case=False, na=False)].copy()
    forwards_df = df[
        df['position'].str.contains("Winger", case=False, na=False) |
        df['position'].str.contains("Forward", case=False, na=False)
    ].copy()

    return defenders_df, midfielders_df, goalkeepers_df, forwards_df


df, mf, gk, fw = split_by_position(file_path)

print("Defenders shape:", df.shape)
print("Midfielders shape:", mf.shape)
print("Goalkeepers shape:", gk.shape)
print("Forwards shape:", fw.shape)

"""---

## General Functions
"""

# def clean_data_basic(df):
#     missing_percentage = df.isnull().sum() / len(df) * 100
#     df = df.loc[:, missing_percentage <= 80]
#     drop_cols = ["name", "id", "Unnamed: 0", "Age", "Pos", "Born", "status", "signedFrom"]

#     for col in df.columns:
#         if df[col].dtype in ['int64', 'float64']:
#             if missing_percentage[col] < 5:
#                 df[col] = df[col].fillna(df[col].median())
#             else:
#                 df[col] = df[col].fillna(df[col].mean())
#         elif df[col].dtype == 'object':
#             df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown')
#         elif np.issubdtype(df[col].dtype, np.datetime64):
#             df[col] = df[col].fillna(pd.Timestamp('1970-01-01'))

#     df.columns = [re.sub(r"[^a-zA-Z0-9_/% ]", "_", col) for col in df.columns]

#     df['marketValue'] = pd.to_numeric(df['marketValue'], errors='coerce')
#     df.drop(columns=drop_cols, inplace=True, errors="ignore")
#     df = df.reset_index(drop=True)

#     df["contract_length"] = (
#         pd.to_datetime(df["contract"]) - pd.to_datetime(df["joinedOn"])
#     ).dt.days.fillna(0)
#     df["year_joined"] = pd.to_datetime(df["joinedOn"]).dt.year.fillna(0)
#     df["month_joined"] = pd.to_datetime(df["joinedOn"]).dt.month.fillna(0)
#     df["age_at_joining"] = df["year_joined"] - pd.to_datetime(df["dateOfBirth"]).dt.year.fillna(0)
#     df["contract_remaining"] = df["contract_length"] / 365

#     if 'age' in df.columns:
#         df['age'] = winsorize(df['age'], limits=[0.05, 0.05])
#     if 'marketValue' in df.columns:
#         df['marketValue'] = winsorize(df['marketValue'], limits=[0.05, 0.05])

#     numerical_cols = df.select_dtypes(include=np.number).columns.tolist()
#     for col in numerical_cols:
#         if col not in ['marketValue']:
#             Q1 = df[col].quantile(0.25)
#             Q3 = df[col].quantile(0.75)
#             IQR = Q3 - Q1
#             lower_bound = Q1 - 1.5 * IQR
#             upper_bound = Q3 + 1.5 * IQR
#             df[col] = np.clip(df[col], lower_bound, upper_bound)

#     df = df.replace([np.inf, -np.inf], np.nan)

#     return df

def clean_data_basic(df):
    # Step 1: Remove columns with more than 80% missing values
    missing_percentage = df.isnull().sum() / len(df) * 100
    df = df.loc[:, missing_percentage <= 80]

    # Drop unnecessary columns
    drop_cols = ["name", "id", "Unnamed: 0", "Age",
                 "Pos", "Born", "status", "signedFrom"]
    df.drop(columns=drop_cols, inplace=True, errors="ignore")

    # Step 2: Fill missing values based on data type
    for col in df.columns:
        if df[col].dtype in ['int64', 'float64']:  # Numeric columns
            if missing_percentage[col] < 5:
                df[col] = df[col].fillna(df[col].median())
            else:
                df[col] = df[col].fillna(df[col].mean())
        elif df[col].dtype == 'object':  # Categorical columns
            df[col] = df[col].fillna(
                df[col].mode()[0] if not df[col].mode().empty else 'Unknown')
        elif np.issubdtype(df[col].dtype, np.datetime64):  # Date columns
            df[col] = df[col].fillna(pd.Timestamp('1970-01-01'))

    # Standardize column names
    df.columns = [re.sub(r"[^a-zA-Z0-9_/% ]", "_", col) for col in df.columns]

    # Convert 'marketValue' to numeric
    df['marketValue'] = pd.to_numeric(df['marketValue'], errors='coerce')

    # Reset index
    df = df.reset_index(drop=True)

    # Step 3: Feature Engineering for Dates
    df["contract_length"] = (
        pd.to_datetime(df["contract"]) - pd.to_datetime(df["joinedOn"])
    ).dt.days.fillna(0)

    df["year_joined"] = pd.to_datetime(df["joinedOn"]).dt.year.fillna(0)
    df["month_joined"] = pd.to_datetime(df["joinedOn"]).dt.month.fillna(0)
    df["age_at_joining"] = df["year_joined"] - \
        pd.to_datetime(df["dateOfBirth"]).dt.year.fillna(0)
    df["contract_remaining"] = df["contract_length"] / 365

    # Step 4: Handle outliers using Winsorization
    if 'age' in df.columns:
        df['age'] = winsorize(df['age'], limits=[0.05, 0.05])
    if 'marketValue' in df.columns:
        df['marketValue'] = winsorize(df['marketValue'], limits=[0.05, 0.05])

    # Step 5: Remove extreme outliers using IQR method
    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()
    for col in numerical_cols:
        if col not in ['marketValue']:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            df[col] = np.clip(df[col], lower_bound, upper_bound)

    # Step 6: Replace infinite values and remove any remaining NaN values
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.dropna().reset_index(drop=True)

    return df

# def preprocess_football_data(df, target_col="marketValue"):

#     drop_cols = ["name", "id", "Unnamed: 0", "Age", "Pos", "Born", "status", "signedFrom"]
#     df.drop(columns=drop_cols, inplace=True, errors="ignore")
#     df = df.reset_index(drop=True)

#     df["contract_length"] = (
#         pd.to_datetime(df["contract"]) - pd.to_datetime(df["joinedOn"])
#     ).dt.days.fillna(0)
#     df["year_joined"] = pd.to_datetime(df["joinedOn"]).dt.year.fillna(0)
#     df["month_joined"] = pd.to_datetime(df["joinedOn"]).dt.month.fillna(0)
#     df["age_at_joining"] = df["year_joined"] - pd.to_datetime(df["dateOfBirth"]).dt.year.fillna(0)
#     df["contract_remaining"] = df["contract_length"] / 365

#     # df["PassAccuracy"] = df["Cmp"] / df["Att_passing"]
#     # df["ShotAccuracy"] = df["SoT"] / df["Sh"]
#     # df["TacklesPer90"] = df["Tkl"] / df["90s"]
#     # df["InterceptionsPer90"] = df["Int"] / df["90s"]
#     # df["ProgressivePassRatio"] = df["PrgP"] / df["Att_passing"]
#     # df["ProgressiveCarriesRatio"] = df["PrgC"] / df["Carries"]

#     df = df.replace([np.inf, -np.inf], np.nan)

#     return df

# df = pd.read_csv(r"C:\Users\Aaryan\Desktop\Vedant\merged-tm-fb.csv")
# processed_df = preprocess_football_data(df)

def prepare_data_with_label_encoding(csv_path: str | None = None, df: pd.DataFrame | None = None, path: bool = False, target_col: str = "marketValue"):
    if path:
        df = pd.read_csv(csv_path)
    elif df is None:
        raise ValueError("Either csv_path or df must be provided.")

    df = df.dropna(subset=[target_col])
    df.columns = [re.sub(r"[^a-zA-Z0-9_]", "_", col) for col in df.columns]

    label_encoders = {}
    for col in df.select_dtypes(include=['object']).columns:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))
        label_encoders[col] = le

    X = df.drop(columns=[target_col])
    y = df[target_col]

    return train_test_split(X, y, test_size=0.2, random_state=42), label_encoders

clean_data = clean_data_basic(final_df)

sum(clean_data.isna().sum())

"""---

## XGB on uncleaned data
"""

def train_xgb(csv_path=None, df=None, path=False, target_col="marketValue"):
    (splits, label_encoders) = prepare_data_with_label_encoding(
        df=df, target_col="marketValue", path=path, csv_path=csv_path)
    X_train, X_test, y_train, y_test = splits

    dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)
    dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)

    params = {
        "objective": "reg:squarederror",
        "max_depth": 10,
        "eta": 0.1,
        # "tree_method": "gpu_hist"
    }

    model = xgb.train(params, dtrain, num_boost_round=200, verbose_eval=50)

    y_pred = model.predict(dtest)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    accuracy = 1 - (mae / np.mean(y_test))
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    print("Mean Absolute Error:", mae)
    print("RÂ² Score:", r2)
    print("Accuracy:", accuracy)
    print("Root Mean Squared Error:", rmse)

train_xgb(csv_path=file_path, path=True, target_col="marketValue")

"""#### Running it individually on the filtered datasets

"""

print("Defenders:")
train_xgb(df=df)
print("\nMidfielders:")
train_xgb(df=mf)
print("\nGoalkeepers:")
train_xgb(df=gk)
print("\nForwards:")
train_xgb(df=fw)

"""---

## XGB with Cross-Validation
"""

def cv_xgb(csv_path=None, df=None, path=False, target_col="marketValue"):
    (splits, label_encoders) = prepare_data_with_label_encoding(
        df=df, target_col="marketValue", path=path, csv_path=csv_path)
    X_train, X_test, y_train, y_test = splits

    dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)
    dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)

    params = {
        "objective": "reg:squarederror",
        "max_depth": 6,
        "eta": 0.1,
        # "tree_method": "gpu_hist"
    }

    results = xgb.cv(params, dtrain, num_boost_round=500, verbose_eval=50)
    display(results.head())
    best_rmse = results['test-rmse-mean'].min()
    print("Best RMSE:", best_rmse)

cv_xgb(csv_path=file_path, path=True, target_col="marketValue")

print("Defenders:")
cv_xgb(df=df)
print("\nMidfielders:")
cv_xgb(df=mf)
print("\nGoalkeepers:")
cv_xgb(df=gk)
print("\nForwards:")
cv_xgb(df=fw)

"""---

## Functions for multiple models
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import cross_val_score


def evaluate_model(model, X_train, X_test, y_train, y_test):
    """
    Evaluates the model for potential overfitting by computing key regression metrics.

    Parameters:
    - model: Trained regression model
    - X_train, X_test: Train and test features
    - y_train, y_test: True labels for training and testing

    Prints RÂ² scores, MSE, MAE, cross-validation scores, and plots residuals.
    """

    # Predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # 1. RÂ² Score
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)

    # 2. Mean Squared Error (MSE)
    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)

    # 3. Root Mean Squared Error (RMSE)
    train_rmse = np.sqrt(train_mse)
    test_rmse = np.sqrt(test_mse)

    # 4. Mean Absolute Error (MAE)
    train_mae = mean_absolute_error(y_train, y_train_pred)
    test_mae = mean_absolute_error(y_test, y_test_pred)

    # 5. Cross-Validation Score (5-Fold)
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')

    # Print Results
    print("ðŸ“Š Model Evaluation Results:")
    print(f"ðŸ”¹ RÂ² Score - Train: {train_r2:.4f}, Test: {test_r2:.4f}")
    print(f"ðŸ”¹ MSE - Train: {train_mse:.4f}, Test: {test_mse:.4f}")
    print(f"ðŸ”¹ RMSE - Train: {train_rmse:.4f}, Test: {test_rmse:.4f}")
    print(f"ðŸ”¹ MAE - Train: {train_mae:.4f}, Test: {test_mae:.4f}")
    print(
        f"ðŸ”¹ Cross-Validation RÂ² (Mean): {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

    # 6. Residual Analysis
    residuals = y_test - y_test_pred
    plt.figure(figsize=(8, 5))
    plt.scatter(y_test_pred, residuals, color='blue', alpha=0.6)
    plt.axhline(y=0, color='red', linestyle='--')
    plt.xlabel("Predicted Values")
    plt.ylabel("Residuals")
    plt.title("Residual Plot")
    plt.show()

def train_and_evaluate(df, model_name):

    (splits, label_encoders) = prepare_data_with_label_encoding(
        df=df, target_col="marketValue", path=False, csv_path=None)
    X_train, X_test, y_train, y_test = splits

    models = {
        "LinearRegression": LinearRegression(),
        "RidgeRegression": Ridge(alpha=1.0),
        "LassoRegression": Lasso(alpha=1.0),
        "ElasticNet": ElasticNet(alpha=1.0, l1_ratio=0.5),

        "XGBoost": xgb.XGBRegressor(objective="reg:squarederror", max_depth=6, eta=0.1, n_estimators=500, enable_categorical=True,
                                    # tree_method="gpu_hist"
                                    ),
        "LightGBM": lgb.LGBMRegressor(objective="regression", num_leaves=31, learning_rate=0.05, n_estimators=100, verbose=-1),
        "RandomForest": RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),
        "GradientBoosting": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),
        "AdaBoost": AdaBoostRegressor(n_estimators=100, random_state=42),
        # "SVR": SVR(kernel='rbf', C=1.0, epsilon=0.1),
        # "MLPRegressor": MLPRegressor(hidden_layer_sizes=(500,), activation='relu', solver='adam', random_state=42, max_iter=300),
    }

    model = models[model_name]
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    print(f"{model_name} -> RMSE: {rmse:.2f}, RÂ²: {r2:.4f}")
    evaluate_model(model, X_train, X_test, y_train, y_test)

    return model, rmse, r2

def train_and_evaluate_ensemble(df, method="stacking"):
    # Prepare data
    (splits, label_encoders) = prepare_data_with_label_encoding(
        df=df, target_col="marketValue", path=False, csv_path=None
    )
    X_train, X_test, y_train, y_test = splits

    # Define base models
    base_models = {
        "Ridge": Ridge(alpha=1.0),
        "Lasso": Lasso(alpha=1.0),
        "ElasticNet": ElasticNet(alpha=1.0, l1_ratio=0.5),
        "RandomForest": RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),
        "XGBoost": xgb.XGBRegressor(objective="reg:squarederror", max_depth=6, eta=0.1, n_estimators=500, enable_categorical=True,
                                    # tree_method="gpu_hist"
                                    ),
        "LightGBM": lgb.LGBMRegressor(objective="regression", num_leaves=31, learning_rate=0.05, n_estimators=100, verbose=-1),
        "GradientBoosting": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),
        "AdaBoost": AdaBoostRegressor(n_estimators=100, random_state=42),
        "SVR": SVR(kernel='rbf', C=1.0, epsilon=0.1),
    }

    if method == "stacking":
        meta_learner = RandomForestRegressor(
            n_estimators=50, max_depth=5, random_state=42)
        model = StackingRegressor(
            estimators=[(name, model) for name, model in base_models.items()],
            final_estimator=meta_learner
        )

    elif method == "bagging":
        # Bagging model using Random Forest
        model = BaggingRegressor(estimator=RandomForestRegressor(n_estimators=100, random_state=42),
                                 n_estimators=10, random_state=42, oob_score=True)

    elif method == "boosting":
        # Voting model that averages predictions from boosting models
        model = VotingRegressor(estimators=[
            ("XGBoost", base_models["XGBoost"]),
            ("LightGBM", base_models["LightGBM"]),
            ("GradientBoosting", base_models["GradientBoosting"])
        ])

    else:
        raise ValueError(
            "Invalid ensemble method. Choose from ['stacking', 'bagging', 'boosting'].")

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    print(f"{method.capitalize()} Ensemble -> RMSE: {rmse:.2f}, RÂ²: {r2:.4f}")
    evaluate_model(model, X_train, X_test, y_train, y_test)

    return model, X_train, X_test, y_test

"""---

## Running each model individually
"""

def run_models_on_position(df):
    df_defenders, df_midfielders, df_forwards, df_goalkeepers = split_by_position(
        df=df)

    models = {}

    for position, df_position in zip([
            "Defenders", "Midfielders", "Forwards", "Goalkeepers"],
            [df_defenders, df_midfielders, df_forwards, df_goalkeepers]):

        if df_position.empty:
            continue

        best_model, best_rmse, best_r2 = None, float("inf"), -float("inf")
        for model_name in [
            "LinearRegression",
            "RidgeRegression",
            "LassoRegression",
            "ElasticNet",
            "XGBoost",
            "LightGBM",
            "RandomForest",
            "GradientBoosting",
            "AdaBoost",
            # "SVR",
            # "MLPRegressor"
        ]:
            print(f"Training and Evaluating: {model_name}")
            model, rmse, r2 = train_and_evaluate(df_position, model_name)
            if rmse < best_rmse:
                best_model, best_rmse = model, rmse
                best_r2 = r2

        print("\n----- Best Model Summary -----")
        print(f"Best Model: {type(best_model).__name__}")
        print(f"Best RMSE: {best_rmse:.2f}")
        print(f"Best RÂ²: {best_r2:.4f}")

        models[position] = [best_model, f"{best_r2:.2f}", f"{best_rmse:.2f}"]
        print(
            f"\nBest model for {position}: {type(best_model).__name__} (RMSE: {best_rmse:.2f}, RÂ²: {best_r2:.4f})\n")

    return models

run_models_on_position(clean_data)

"""---

## Ensemble learning
"""

processed_df = clean_data_basic(clean_data)
train_and_evaluate_ensemble(processed_df, method="stacking")
train_and_evaluate_ensemble(processed_df, method="bagging")
train_and_evaluate_ensemble(processed_df, method="boosting")

"""---

## Feature Engineering
"""

def add_pos_metrics(df, pos, drop=False):

    if (pos == 'Defender'):
        # Defensive Actions
        tackles = np.array(df["Tkl"])
        interceptions = np.array(df["Int"])
        blocks = np.array(df["Blocks"])
        clearances = np.array(df["Clr"])
        recoveries = np.array(df["Recov"])
        df["Defensive Actions"] = (
            tackles+interceptions+blocks+clearances+recoveries)/90
        # Aerial Duels
        df["Aerial Duels"] = np.array(df["Won%"])/90
        # Passing and Build-up Play
        key_passes = np.array(df["KP"])
        pass_cmp = np.array(df["Cmp%"])
        df["Passing and Build-up"] = (key_passes+pass_cmp)/90
        # Positioning and Defensive Awarness
        df["Defensive Awareness"] = (clearances+blocks)/90
        # Defensive Contributions
        yellow_cards = np.array(df["CrdY"])
        second_yellow_card = np.array(df["2CrdY"])
        red_cards = np.array(df["CrdR"])
        fouls = np.array(df["Fls"])
        df["Discipline"] = (yellow_cards+second_yellow_card+red_cards+fouls)/90

        # Defensive Duties
        tackles_Def_3rd = np.array(df["Def 3rd"])
        interceptions = np.array(df["Int"])
        df["Defensive Duties"] = (tackles_Def_3rd+interceptions)/90
        # Offensive Contribution
        prg_carries = np.array(df["PrgC"])
        prg_passes = np.array(df["PrgP"])
        key_passes = np.array(df["KP"])
        xA = np.array(df["xA"])
        df["Offensive Contributions"] = (
            prg_carries+prg_passes+xA+key_passes)/90
        # Final Third Play
        crosses_attempted = np.array(df["Crs"])
        shot_creating_actions = np.array(df["SCA"])
        carries_penalty_area = np.array(df["CPA"])
        df["Final Third Play"] = (
            crosses_attempted+shot_creating_actions+carries_penalty_area)/90
        # Possession Play
        TAtt3rd = np.array(df["Att 3rd_possession"])
        carry_distance = np.array(df["TotDist"])
        df["Possession Play"] = (TAtt3rd+carry_distance)/90
        # Dribbling and Transition Play
        successful_take_ons = np.array(df["Succ"])
        df["Dribbling"] = successful_take_ons/90
        if drop:
            df = df.drop(["Tkl", "Int", "Blocks", 'Clr', 'Recov', 'Won%', 'KP', 'Cmp%', 'CrdY', '2CrdY', 'CrdR', 'Fls', 'Def 3rd',
                         'Int', 'PrgC', 'PrgP', 'KP', 'xA', 'Crs', 'SCA', 'CPA', 'Att 3rd_possession', 'TotDist', 'Succ', 'Dribbling'], axis=1)

    elif (pos == 'Midfielder'):
        # Defensive Contributions
        tackles = np.array(df["Tkl"])
        interceptions = np.array(df["Int"])
        blocks = np.array(df["Blocks"])
        clearances = np.array(df["Clr"])
        recoveries = np.array(df["Recov"])
        df["Defensive Contributions"] = (
            tackles+interceptions+blocks+clearances+recoveries)/90
        # Passing Ability
        pass_cmp = np.array(df["Cmp%"])
        df["Passing Ability"] = pass_cmp/90
        # Build-up Play
        xA = np.array(df["xA"])
        xAG = np.array(df["xAG"])
        npxG = np.array(df["npxG"])
        df["Build-Up Play"] = (xA+xAG+npxG)/90
        # Ball Recovery and Defensive Work
        recoveries = np.array(df["Recov"])
        interceptions = np.array(df["Int"])
        df["Ball Recovery & Defensive Work"] = (recoveries+interceptions)/90
        # Defensive Line Breaking Passes
        prg_passes = np.array(df["PrgP"])
        key_passes = np.array(df["KP"])
        passes_final_third = np.array(df["1/3"])
        df["Line Breaking Passes"] = (
            key_passes+prg_passes+passes_final_third)/90

        # Passing and Vision
        prg_passes = np.array(df["PrgP"])
        passes_final_third = np.array(df["1/3"])
        df["Passing"] = (prg_passes+passes_final_third)/90
        # Ball Carrying
        successful_take_ons = np.array(df["Succ"])
        prg_carries = np.array(df["PrgC"])
        cpa = np.array(df["CPA"])
        df["Ball Carrying"] = (successful_take_ons+prg_carries+cpa)/90
        # Defensive Work
        tackles = np.array(df["Tkl"])
        interceptions = np.array(df["Int"])
        blocks = np.array(df["Blocks"])
        clearances = np.array(df["Clr"])
        recoveries = np.array(df["Recov"])
        df["Defensive Work"] = (tackles+interceptions +
                                blocks+clearances+recoveries)/90
        # Chance Creation
        sca = np.array(df["SCA"])
        xG = np.array(df["xG"])
        xA = np.array(df["xA"])
        xAG = np.array(df["xAG"])
        df["Chance Creation"] = (sca+xG+xA+xAG)/90
        # Possession Retention
        pass_cmp = np.array(df["Cmp"])
        key_passes = np.array(df["KP"])
        passes_final_third = np.array(df["1/3"])
        successful_take_ons = np.array(df["Succ"])
        df["Possession Retention"] = (
            pass_cmp+key_passes+passes_final_third+successful_take_ons)/90

        # Creativity and Playmaking
        xA = np.array(df["xA"])
        sca = np.array(df["SCA"])
        passes_final_third = np.array(df["1/3"])
        df["Playmaking"] = (xA+sca+passes_final_third)/90
        # Ball Progression
        prg_carries = np.array(df["PrgC"])
        prg_passes = np.array(df["PrgP"])
        df["Ball Progression"] = (prg_passes+prg_carries)/90
        # Final Third Impact
        TAtt3rd = np.array(df["Att 3rd_possession"])
        cpa = np.array(df["CPA"])
        ppa = np.array(df["PPA"])
        df["Final Third Impact"] = (TAtt3rd+cpa+ppa)/90
        # Goal Threat
        xG = np.array(df["xG"])
        npxG = np.array(df["npxG"])
        goal = np.array(df["Gls"])
        df["Goal Threat"] = (xG+npxG+goal)/90
        # Final Ball Efficiency
        passes_final_third = np.array(df["1/3_possession"])
        ppa = np.array(df['PPA'])
        df["Final Ball Efficiency"] = (passes_final_third+ppa)/90

    elif (pos == "Forward"):
        # Dribbling and Ball Carrying
        successful_take_ons = np.array(df["Succ"])
        prg_carries = np.array(df["PrgC"])
        cpa = np.array(df["CPA"])
        df["Dribbling"] = (successful_take_ons+prg_carries+cpa)/90
        # Crossing and Playmaking
        xA = np.array(df["xA"])
        xAG = np.array(df["xAG"])
        crosses_attempted = np.array(df["Crs"])
        df["Crosses and Playmaking"] = (xA+xAG+crosses_attempted)/90
        # Goal Threat
        xG = np.array(df["xG"])
        npxG = np.array(df["npxG"])
        goal = np.array(df["Gls"])
        df["Goal Threat"] = (xG+npxG+goal)/90
        # Final Third Involvement
        TAtt3rd = np.array(df["Att 3rd_possession"])
        cpa = np.array(df["CPA"])
        ppa = np.array(df["PPA"])
        df["Final Third Impact"] = (TAtt3rd+cpa+ppa)/90
        # Ball Carrying
        successful_take_ons = np.array(df["Succ"])
        prg_carries = np.array(df["PrgC"])
        cpa = np.array(df["CPA"])
        df["Ball Carrying"] = (successful_take_ons+prg_carries+cpa)/90

        # Goal Threat
        xG = np.array(df["xG"])
        npxG = np.array(df["npxG"])
        goal = np.array(df["Gls"])
        df["Goal Threat"] = (xG+npxG+goal)/90
        # Chance Conversion
        npGoals = np.array(df["G-PK"])/90
        xG = np.array(df["xG"])/90
        df["Chance Conversion"] = npGoals/xG
        # Link-up Play
        prg_received = np.array(df["PrgR"])
        passes_final_third = np.array(df["1/3_possession"])
        xA = np.array(df["xA"])
        ppa = np.array(df["PPA"])
        df["Link-Up Play"] = (prg_received+xA+ppa)/90
        # Shooting Accuracy
        SoT = np.array(df["SoT/90"])
        shots = np.array(df["Sh/90"])
        df["Shooting Accuracy"] = SoT+shots
        # Penalty Box Presence
        TAttPen = np.array(df["Att Pen"])
        df["Penalty Box Presence"] = TAttPen/90

    elif (pos == "Goalkeeper"):
        # Shot Stopping Ability
        saves = np.array(df["Saves"])
        df["Shot Stopping"] = saves/90
        # Expected Goals Prevented
        psxG = np.array(df["PSxG"]/90)
        ga = np.array(df["GA"]/90)
        df["Expected Goals Prevented"] = psxG-ga
        # Cross and Aerial Control
        crosses_stopped = np.array(df["Stp"])
        df["Cross and Aerial Control"] = crosses_stopped/90
        # Sweeper Keeper Activity
        sweeper = np.array(df["#OPA/90"])
        df["Sweeper Keeper Activity"] = sweeper
        # Passing and Distribution
        passes_cmp = np.array(df["Cmp"])
        df["Passing"] = passes_cmp/90

    return df

# prompt: use add_pos_metrics to get feature engineered df for the 4 positions defined, then perform extensive EDA one each one of them

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'clean_data' is your DataFrame and 'add_pos_metrics' is defined as in your provided code.

def perform_eda(df, position):
    print(f"\n--- Exploratory Data Analysis for {position} ---\n")

    # 1. Summary Statistics
    print("Summary Statistics:")
    print(df.describe())

    # 2. Data Types and Missing Values
    print("\nData Types and Missing Values:")
    print(df.info())

    # 3. Histograms for Numerical Features
    print("\nHistograms of Numerical Features:")
    numerical_features = df.select_dtypes(include=np.number).columns
    for feature in numerical_features:
        plt.figure(figsize=(8, 6))
        sns.histplot(df[feature], kde=True)
        plt.title(f"Histogram of {feature} for {position}")
        plt.show()

    # 4. Box Plots for Numerical Features (to visualize outliers)
    print("\nBoxplots of Numerical Features:")
    for feature in numerical_features:
        plt.figure(figsize=(8, 6))
        sns.boxplot(x=df[feature])
        plt.title(f"Boxplot of {feature} for {position}")
        plt.show()


    # 5. Correlation Matrix
    print("\nCorrelation Matrix:")

    # Select only numeric columns
    numeric_df = df.select_dtypes(include='number')

    correlation_matrix = numeric_df.corr()
    plt.figure(figsize=(12, 10))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title(f"Correlation Matrix for {position}")
    plt.show()


    # 6. Pairplot for selected numerical features (optional, can be computationally expensive for many features)
    # selected_features = ['marketValue', 'Gls', 'Ast', 'xG', ...]  # Replace with relevant features
    # sns.pairplot(df[selected_features])
    # plt.suptitle(f"Pairplot for {position}")
    # plt.show()


def analyze_positions(df):
    df_defenders, df_midfielders, df_forwards, df_goalkeepers = split_by_position(df=df)

    positions_data = {
        'Defenders': df_defenders,
        'Midfielders': df_midfielders,
        'Forwards': df_forwards,
        'Goalkeepers': df_goalkeepers
    }

    for position, position_df in positions_data.items():
        if not position_df.empty:
            position_df = add_pos_metrics(position_df.copy(), position)  # .copy() to avoid SettingWithCopyWarning
            perform_eda(position_df, position)

# Example usage
analyze_positions(clean_data) # Assuming clean_data is your dataframe



def feature_engineered_indi_model(df):

    models = {}

    dfs, mf, gk, fw = split_by_position(df=df)

    for df, pos in zip([dfs, mf, fw, gk], ["Defender", "Midfielder", "Forward", "Goalkeeper"]):
        df = add_pos_metrics(df, pos)
        df = clean_data_basic(df)
        # print(sum(df.isna().sum()))

    for position, df_position in zip([
            "Defenders", "Midfielders", "Forwards", "Goalkeepers"],
            [dfs, mf, fw, gk]):

        if df_position.empty:
            continue

        # df_position = clean_data_basic(df_position)
        best_model, best_rmse, best_r2 = None, float("inf"), -float("inf")

        for model_name in [
            # "LinearRegression",
            # "RidgeRegression",
            # "LassoRegression",
            # "ElasticNet",
            "XGBoost",
            "LightGBM",
            "RandomForest",
            # "GradientBoosting",
            # "AdaBoost",
            # "SVR",
            # "MLPRegressor"
        ]:
            model, rmse, r2 = train_and_evaluate(df_position, model_name)
            if rmse < best_rmse:
                best_model, best_rmse, best_r2 = model, rmse, r2

        models[position] = best_model
        print(
            f"\nBest model for {position}: {type(best_model).__name__} (RMSE: {best_rmse:.2f}, RÂ²: {best_r2:.4f})\n")

    return models


feature_engineered_indi_model(final_df)

for pos, df in zip(["Defender", "Midfielder", "Forward", "Goalkeeper"], [df, mf, fw, gk]):
    df = add_pos_metrics(df, pos)
    # df = clean_data_basic(df)
    stacking = train_and_evaluate_ensemble(df, method="stacking")
    bagging = train_and_evaluate_ensemble(df, method="bagging")
    boosting = train_and_evaluate_ensemble(df, method="boosting")

    print(
        f"Position: {pos}:\n\tStacking: {stacking}\n\tBagging: {bagging}\n\tBoosting: {boosting}")

"""---

## PCA Inclusion
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler


def find_optimal_components(df, variance_threshold=0.95):
    # df = clean_data_basic(df)
    # Select numeric data
    #
    numeric_df = df.select_dtypes(include=['number'])

    # Standardize the data
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(numeric_df)

    # Perform PCA
    pca = PCA()
    pca.fit(scaled_data)

    # Compute cumulative explained variance
    explained_variance = np.cumsum(pca.explained_variance_ratio_)

    # Find the number of components that explain the desired variance
    optimal_components = np.argmax(
        explained_variance >= variance_threshold) + 1

    # Plot explained variance
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, len(explained_variance) + 1),
             explained_variance, marker='o', linestyle='--', color='b')
    plt.axhline(y=variance_threshold, color='r', linestyle='--')
    plt.xlabel('Number of Components')
    plt.ylabel('Cumulative Explained Variance')
    plt.title('Explained Variance vs. Number of Components')
    plt.grid()
    plt.show()

    return optimal_components

optimal_n = find_optimal_components(clean_data, variance_threshold=0.95)
print(f"Optimal number of components: {optimal_n}")

eigenvalues = PCA().fit(StandardScaler().fit_transform(
    clean_data.select_dtypes(include=['number']))).explained_variance_
optimal_n_kaiser = sum(eigenvalues > 1)
print(f"Optimal components based on Kaiser's Rule: {optimal_n_kaiser}")

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler


def perform_pca(df, n_components=2):
    """
    Performs PCA on the given DataFrame and returns the transformed DataFrame.

    Parameters:
        df (pd.DataFrame): The input DataFrame with numerical values.
        n_components (int): The number of principal components to keep.

    Returns:
        pd.DataFrame: Transformed DataFrame with principal components.
        PCA: The trained PCA model.
    """

    # Drop non-numeric columns
    numeric_df = df.select_dtypes(include=['number'])

    # Standardize the data
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(numeric_df)

    # Apply PCA
    pca = PCA(n_components=n_components)
    pca_transformed = pca.fit_transform(scaled_data)

    # Create a DataFrame with principal components
    pca_df = pd.DataFrame(pca_transformed, columns=[
                          f'PC{i+1}' for i in range(n_components)])
    marketValue = df["marketValue"]
    # Retain non-numeric columns for reference
    non_numeric_df = df.select_dtypes(
        exclude=['number']).reset_index(drop=True)
    result_df = pd.concat([non_numeric_df, pca_df], axis=1)
    result_df["marketValue"] = marketValue
    return result_df, pca


reduced_df, pca_model = perform_pca(clean_data, n_components=35)
reduced_df.head()

"""---

## PCA with individual models
"""

def pca_on_individual_model(df):
    models = {}

    df, mf, gk, fw = split_by_position(df=df)

    # for df, pos in zip([df, mf, fw, gk], ["Defender", "Midfielder", "Forward", "Goalkeeper"]):
    # df = add_pos_metrics(df, pos)

    for position, df_position in zip([
            "Defenders", "Midfielders", "Forwards", "Goalkeepers"],
            [df, mf, fw, gk]):

        if df_position.empty:
            continue

        df_position = clean_data_basic(df_position)
        best_model, best_rmse, best_r2 = None, float("inf"), -float("inf")

        for model_name in [
            "LinearRegression", "RidgeRegression", "LassoRegression",
            "ElasticNet",
            "XGBoost", "LightGBM", "RandomForest", "GradientBoosting",
                "AdaBoost"]:
            model, rmse, r2 = train_and_evaluate(df_position, model_name)
            if rmse < best_rmse:
                best_model, best_rmse, best_r2 = model, rmse, r2

        models[position] = best_model
        print(
            f"\nBest model for {position}: {type(best_model).__name__} (RMSE: {best_rmse:.2f}, RÂ²: {best_r2:.4f})\n")

    return models



pca_on_individual_model(reduced_df)

"""---

## PCA + Ensemble Learning
"""

train_and_evaluate_ensemble(reduced_df, method="stacking")
train_and_evaluate_ensemble(reduced_df, method="bagging")
train_and_evaluate_ensemble(reduced_df, method="boosting")

"""---

"""

import shap
from sklearn.metrics import classification_report, confusion_matrix

model, X_train, X_test, y_test = train_and_evaluate_ensemble(
    reduced_df, method="stacking")

explainer = shap.Explainer(model.predict, X_train)

shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test)

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values, X_test, plot_type="bar")
plt.title("SHAP Summary Plot (Bar)")
plt.show()

# Create a dependence plot (example for PC1)
plt.figure(figsize=(10, 6))  # Adjust figure size
shap.dependence_plot("PC1", shap_values.values, X_test, interaction_index="PC2")
plt.title("SHAP Dependence Plot (PC1 vs PC2)")
plt.show()


# Iterate over the estimators within the StackingRegressor
for i, estimator in enumerate(model.estimators_):
    # Create explainer
    explainer_i = shap.Explainer(estimator.predict, X_train)

    # Calculate SHAP values
    shap_values_i = explainer_i(X_test)

    # Summary plot for each estimator
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values_i, X_test, show=False)
    plt.title(f"SHAP Summary Plot for Estimator {i + 1}: {type(estimator).__name__}")
    plt.show()



import shap
from sklearn.metrics import classification_report, confusion_matrix

model, X_train, X_test, y_test = train_and_evaluate_ensemble(
    clean_data, method="stacking")

# prompt: load model using pickle

import pickle

# Load the saved model
with open('/content/football_model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

# Now you can use the loaded_model to make predictions
# Example:
# predictions = loaded_model.predict(new_data)

